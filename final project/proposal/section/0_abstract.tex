\begin{abstract}

Recent years have witnessed a paradigm shift in 3D computer vision, moving from per-scene optimization methods to generalizable 3D foundation models. Unlike previous approaches that require training on individual scenes (e.g., NeRF\cite{Nerf}, Gaussian Splatting\cite{3dgs}), these emerging models leverage large-scale pre-training and Transformer architectures to perform 3D tasks in a robust, zero-shot manner. This project proposes a comprehensive review of this development. I will analyze 4 representative papers published in 2024-2025, focusing on two key pillars: geometric reconstruction (DUSt3R, VGGT) and novel view synthesis (LVSM, RayZer). The report aims to discuss the architectural innovations that enable these models to serve as general-purpose 3D vision backbones.

\end{abstract}