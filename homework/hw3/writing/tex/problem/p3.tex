\begin{homeworkProblem}

3: Policy Gradient Method (7 pts)

Consider a discounted Markov Decision Process (MDP) $(\mS,\A,P,r,\gamma)$ with states $s\in\mS$, actions $a\in\A$, transition kernel $P(s' | s,a)$, reward $r(s,a)$ bounded, and discount $\gamma\in(0,1)$. Let $\pi_{\theta}(a | s)$ be a differentiable, stochastic policy with parameters $\theta$, and let $s_0$ be a fixed start state. Define the (discounted) return
$$G_0 = \sum_{t=0}^{\infty} \gamma^t r(s_t,a_t)$$
and the performance objective
$$J(\theta) = V^{\pi_{\theta}}(s_0) = \E_{\tau\sim \pi_{\theta}}\left[G_0\right]$$
where a trajectory $\tau=(s_0,a_0,s_1,a_1,\ldots)$ is generated by $s_{t+1}\sim P(\cdot | s_t,a_t)$ and $a_t\sim \pi_{\theta}(\cdot | s_t)$.

Denote the value and action-value functions by
$$V^{\pi}(s) = \E_\pi\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t) \Big| s_0=s\right], \qquad Q^{\pi}(s,a) = \E_\pi\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t) \Big| s_0=s, a_0=a\right]$$
and the advantage by $A^{\pi}(s,a)=Q^{\pi}(s,a)-V^{\pi}(s)$. Let $d^{\pi}(s)$ be the (unnormalized) $\gamma$-discounted state visitation distribution:
$$d^{\pi}(s) = \sum_{t=0}^{\infty} \gamma^t \Pr(s_t=s | \pi)$$

\begin{itemize}
\item (4 pts) Prove the Policy Gradient Theorem.
$$\nabla_{\theta} J(\theta) = \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty} \gamma^t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) Q^{\pi_{\theta}}(s_t,a_t) \right] = \frac{1}{1-\gamma} \E_{s\sim d^{\pi_{\theta}}, a\sim \pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(a | s) Q^{\pi_{\theta}}(s,a)  \right]$$

\item (3 pts) Show that for any function $b:\mS\to\mathbb{R}$,
$$\E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)  b(s_t)\right] = 0$$
and hence the policy gradient can be equivalently written as
$$\nabla_{\theta} J(\theta) = \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \left(Q^{\pi_{\theta}}(s_t,a_t)-b(s_t)\right)\right]$$
In the advantage method, how should $b(s)$ be chosen, and what is the benefit of this choice?
\end{itemize}

\textcolor{blue}{\textbf{Solution}}

(1) Let $\tau=(s_0,a_0,s_1,a_1,\ldots)$ be an trajectory with initial state $s_0$. Since it is a Markov model, thus its probability under $\pi_{\theta}$ is
$$p_{\theta}(\tau)=\prod_{t=0}^{\infty}\pi_{\theta}(a_t | s_t) P(s_{t+1} | s_t,a_t)$$
Since the transition probability is independent of $\theta$, thus we have
\begin{align*}
J(\theta) &= \E_{\tau\sim\pi_{\theta}}[G_0] = \int G_0(\tau)p_{\theta}(\tau)\dtau \\
\Rightarrow\quad \nabla_{\theta} J(\theta) &= \int G_0(\tau)\nabla_{\theta} p_{\theta}(\tau)\dtau = \int G_0(\tau)p_{\theta}(\tau)\nabla_{\theta}\log p_{\theta}(\tau)\dtau = \E_{\tau\sim\pi_{\theta}}\left[\nabla_{\theta}\log p_{\theta}(\tau)G_0\right]
\end{align*}

Using the definition of the discounted return, for each $t$, we have
\begin{align*}
G_0 &= \sum_{k=0}^{\infty}\gamma^k r(s_k,a_k) \\
&= \sum_{k=0}^{t-1}\gamma^k r(s_k,a_k) + \sum_{k=t}^{\infty}\gamma^t\gamma^{k-t} r(s_k,a_k) \\
&= \sum_{k=0}^{t-1}\gamma^k r(s_k,a_k) + \gamma^t\sum_{k=0}^{\infty}\gamma^{k} r(s_{k+t},a_{k+t}) \\
&= \underbrace{\sum_{k=0}^{t-1}\gamma^k r(s_k,a_k)}_{=R_{<t}} + \gamma^t G_t
\end{align*}

Then
$$\E_{\tau\sim\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)G_0\right] = \E_{\tau\sim\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)R_{<t}\right] + \E_{\tau\sim\pi_{\theta}}\left[\gamma^t \nabla_{\theta}\log\pi_{\theta}(a_t | s_t)G_t\right]$$
Let the history up to state $s_t$ be $H_t=(s_0,a_0,\ldots,s_t)$. Note that $R_{<t}$ is measurable with respect to $H_t$ and does not depend on $a_t$. Using the law of iterative expectation, we can get that
$$\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)R_{<t}\right] = \E_{\pi_{\theta}}\left[\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)R_{<t} | H_t\right]\right] = \E_{\pi_{\theta}}\left[R_{<t}\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t) | H_t\right]\right]$$

Conditioned on $H_t$, the state $s_t$ is fixed and $a_t\sim \pi_{\theta}(\cdot | s_t)$, thus
$$\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t) | H_t\right] = \sum_{a}\pi_{\theta}(a | s_t)\nabla_{\theta}\log\pi_{\theta}(a | s_t) = \sum_{a}\nabla_{\theta}\pi_{\theta}(a | s_t) = \nabla_{\theta} \sum_{a}\pi_{\theta}(a | s_t) = \nabla_{\theta} 1=0$$
i.e. $\E_{\pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)R_{<t}]=0$, so we can get that
$$\nabla_{\theta} J(\theta) = \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t \nabla_{\theta}\log\pi_{\theta}(a_t | s_t)G_t\right]$$

From the definition of $Q^{\pi_{\theta}}$, we have
$$Q^{\pi_{\theta}}(s_t,a_t)=\E_{\pi_{\theta}}[G_t | s_t,a_t]$$
Using the law of total expectation to the above result again, we can get that
\begin{align*}
\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)G_t\right] &= \E_{\pi_{\theta}}\left[\E_{\pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)G_t | s_t,a_t]\right] \\
&= \E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t) \E_{\pi_{\theta}}[G_t | s_t,a_t]\right] \\
&= \E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)Q^{\pi_{\theta}}(s_t,a_t)\right] \\
\Rightarrow\quad \nabla_{\theta} J(\theta) &= \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t \nabla_{\theta}\log\pi_{\theta}(a_t | s_t)G_t\right] \\
&= \sum_{t=0}^{\infty}\gamma^t \E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)G_t\right] \\
&= \sum_{t=0}^{\infty}\gamma^t \E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)Q^{\pi_{\theta}}(s_t,a_t)\right] \\
&= \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t \nabla_{\theta}\log\pi_{\theta}(a_t | s_t)Q^{\pi_{\theta}}(s_t,a_t)\right]
\end{align*}
Which means that we have proved the first part of the equality.


For the second part, expand the expectation, we can get that for any function $f:\mS\times\mathcal{A}\to\mathbb{R}$:
\begin{align*}
\E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t f(s_t,a_t)\right] &= \sum_{t=0}^{\infty}\gamma^t \sum_{s}\Pr(s_t=s | \pi_{\theta})\sum_{a}\pi_{\theta}(a | s)f(s,a) \\
&= \sum_{s}\sum_{t=0}^{\infty}\gamma^t \Pr(s_t=s | \pi_{\theta})\sum_{a}\pi_{\theta}(a | s)f(s,a) \\
&= \sum_{s}d^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(a | s)f(s,a)
\end{align*}

Put $f(s,a)=\nabla_{\theta}\log\pi_{\theta}(a | s)Q^{\pi_{\theta}}(s,a)$ into above equation, we can get that
$$\nabla_{\theta} J(\theta) = \sum_{s}d^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(a | s)\nabla_{\theta}\log\pi_{\theta}(a | s)Q^{\pi_{\theta}}(s,a)$$
Also, the normalization factor of sampling $s$ from $d^{\pi_{\theta}}$ is
$$\sum_{s}d^{\pi_{\theta}}(s) = \sum_{t=0}^{\infty}\gamma^t \sum_s \Pr(s_t=s | \pi_{\theta})=\sum_{t=0}^{\infty}\gamma^t=\frac{1}{1-\gamma}$$
Which means that the probability of sampling $s$ is
$$\Pr_{s\sim d^{\pi_{\theta}}}(s) = (1-\gamma)d^{\pi_{\theta}}(s)$$
Therefore the previous identity can be rewritten as
\begin{align*}
\nabla_{\theta} J(\theta) &= \sum_{s} d^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(a | s)\nabla_{\theta}\log\pi_{\theta}(a | s)Q^{\pi_{\theta}}(s,a) \\
&= \frac{1}{1-\gamma}\sum_{s} (1-\gamma)d^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(a | s)\nabla_{\theta}\log\pi_{\theta}(a | s)Q^{\pi_{\theta}}(s,a) \\
&= \frac{1}{1-\gamma}\sum_{s} \Pr_{s\sim d^{\pi_{\theta}}}(s)\sum_{a}\pi_{\theta}(a | s)\left[\nabla_{\theta}\log\pi_{\theta}(a | s)Q^{\pi_{\theta}}(s,a)\right] \\
&= \frac{1}{1-\gamma} \E_{s\sim d^{\pi_{\theta}}, a\sim \pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(a | s) Q^{\pi_{\theta}}(s,a)\right]
\end{align*}

So above all, we have proved that
$$\nabla_{\theta} J(\theta) = \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty} \gamma^t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) Q^{\pi_{\theta}}(s_t,a_t) \right] = \frac{1}{1-\gamma} \E_{s\sim d^{\pi_{\theta}}, a\sim \pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(a | s) Q^{\pi_{\theta}}(s,a)\right]$$




(2) Using the law of iterative expectation, we have
$$\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)b(s_t)\right] = \E_{\pi_{\theta}}\left[\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)b(s_t)|s_t\right]\right] = \E_{\pi_{\theta}}\left[b(s_t)\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t) | s_t\right]\right]$$
For any state $s$,
$$\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t) | s_t\right] = \sum_{a}\pi_{\theta}(a | s_t)\nabla_{\theta}\log\pi_{\theta}(a | s_t) = \sum_{a}\nabla_{\theta}\pi_{\theta}(a | s_t) = \nabla_{\theta}\sum_{a}\pi_{\theta}(a | s_t) = \nabla_{\theta} 1=0$$
i.e.
$$\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)b(s_t)\right] = \E_{\pi_{\theta}}\left[b(s_t)\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t) | s_t\right]\right] = \E_{\pi_{\theta}}\left[b(s_t)\cdot 0\right] = 0$$
Thus each term is zero, so summing up, we can get that
$$\E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)b(s_t)\right] = \sum_{t=0}^{\infty}\gamma^t\E_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)b(s_t)\right] = \sum_{t=0}^{\infty}\gamma^t\cdot 0 = 0$$

Subtracting this zero term from the policy gradient, we can get that
\begin{align*}
\nabla_{\theta} J(\theta) &= \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)Q^{\pi_{\theta}}(s_t,a_t)\right] \\
&= \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)Q^{\pi_{\theta}}(s_t,a_t)\right] - \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)b(s_t)\right] \\
&= \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)\left(Q^{\pi_{\theta}}(s_t,a_t)-b(s_t)\right)\right]
\end{align*}

So above all, we have proved that
$$\E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)b(s_t)\right] = 0$$
and
$$\nabla_{\theta} J(\theta) = \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)\left(Q^{\pi_{\theta}}(s_t,a_t)-b(s_t)\right)\right]$$

In the advantage method, choose
$$b(s)=V^{\pi_{\theta}}(s)$$
so that $Q^{\pi_{\theta}}(s,a)-b(s) = Q^{\pi_{\theta}}(s,a) - V^{\pi_{\theta}}(s) = A^{\pi_{\theta}}(s,a)$ and
$$\nabla_{\theta} J(\theta) = \E_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^t\nabla_{\theta}\log\pi_{\theta}(a_t | s_t)A^{\pi_{\theta}}(s_t,a_t)\right]$$
This choice keeps the estimator unbiased and most importantly reducing variance, which makes the training more stable and sample-efficient policy optimization.

\end{homeworkProblem}

\newpage