\section{Introduction}

A long-standing goal in 3D computer vision is to infer scene structure and synthesize novel observations from limited visual measurements. Classical pipelines achieve this goal by combining explicit projective geometry with multi-view constraints, while recent methods often rely on per-scene optimization to obtain high-fidelity reconstructions. Despite strong performance under per-scene settings, these paradigms face persistent obstacles when deployed at scale: sensitivity to camera calibration and pose accuracy, limited transfer across scenes, and substantial computational cost when adapting to new environments. These issues become critical as modern applications demand 3D reasoning and rendering across diverse, unconstrained data distributions.

The recent emergence of large pre-trained models has renewed interest in generalization for 3D vision. These changes indicated that the geometric and photometric regularities can be captured as learned priors from data, rather than being enforced exclusively through hand-designed constraints. However, the extent to which geometry can be learned implicitly remains unclear. In particular, current methods reveal a fundamental tension between the amount and form of supervision, the intermediate representation used to couple views and predict 3D quantities, and the degree of explicit 3D inductive bias retained to ensure consistency and controllability.

This report surveys representative progress from 2024-2025 through four recent methods. For geometric reconstruction, we focus on DUSt3R~\cite{Dust3r} and VGGT~\cite{Vggt}, which pursue generalizable geometry prediction via learned multi-view reasoning and direct 3D regression. For novel view synthesis, we review LVSM~\cite{Lvsm} and RayZer~\cite{RayZer}, which move toward photorealistic synthesis with reduced explicit 3D modeling and, in some cases, weaker 3D supervision. Together, these works provide a compact but informative slice of the emerging design space for general-purpose 3D backbones.

Rather than treating each paper in isolation, we aim to analyze the motivation, core design choices, and key contributions of each method, and then synthesize them under a unified comparison, thereby clarifying which ingredients appear necessary for robust 3D inference and high-quality synthesis.