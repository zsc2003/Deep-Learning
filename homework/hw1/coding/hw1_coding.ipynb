{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Coding Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [15pts] Training a Convolutional Neural Network (CNN) on EuroSAT for Image Classification\n",
    "\n",
    "In this assignment, you will train a deep learning model from scratch for EuroSAT dataset classification. EuroSAT is a dataset of 27,000 RGB satellite images ($64\\times 64$ pixels) across 10 land cover classes, derived from Sentinel-2 satellite data for remote sensing classification tasks.\n",
    "\n",
    "You are required to complete the following code by **filling in your own architecture and training function.** In the sections that specify \"**To be implemented by students**\", you should replace pass with your own implementation.\n",
    "\n",
    "After completing the implementation, answer the following questions and submit a report in Markdown/PDF format.\n",
    "- Estimate the number of parameters and the feature map sizes at each layer.\n",
    "- Report training accuracy and loss over epoches and the testing accuracy on test data.\n",
    "- Compare training and test error with and without Batch Normalization and Dropout layers.\n",
    "- Please also submit the Jupyter Notebook (.ipynb) with your complete, executable code.(You can just edit on this notebook file.)\n",
    "\n",
    "### Notes:\n",
    "- **Google Colab or AutoDL is recommended for training if you donâ€™t have a local GPU.**\n",
    "- **Submission Deadline: November 2, 11:59 PM**\n",
    "\n",
    "The answer to the questions above are in $\\texttt{hw1\\_2025213446.pdf}$'s Problem 4 part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Setup: Load Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "\n",
    "# Ensure GPU usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standardization\n",
    "])\n",
    "\n",
    "# Load EuroSAT dataset\n",
    "dataset = datasets.EuroSAT(root=\"./data\", transform=transform, download=True)\n",
    "\n",
    "# Split dataset into 80% training and 20% testing\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([64, 3, 64, 64]), label size: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Test the input size\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(f\"Input size: {images.size()}, label size: {labels.size()}\")\n",
    "# (B, 3, 64, 64)\n",
    "# (B,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the Neural Network (**To be implemented by students**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a better control whether use BatchNorm and Dropout\n",
    "class MyConvLayer(nn.Module):\n",
    "    def __init__(self, in_c, out_c, use_bn=False, p_drop=0.0):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1, bias=not use_bn)]\n",
    "        if use_bn:\n",
    "            layers.append(nn.BatchNorm2d(out_c, momentum=0.15))\n",
    "        layers.append(nn.ReLU())\n",
    "        if p_drop > 0:\n",
    "            layers.append(nn.Dropout2d(p_drop))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, use_bn=False, p_drop=0.0):\n",
    "        super(MyCNN, self).__init__()\n",
    "        # - Define your CNN model architecture\n",
    "        # - Experiment with different layers, number of filters, kernel sizes\n",
    "        # - Try using BatchNorm, Dropout, and deeper architectures\n",
    "\n",
    "        self.use_bn = use_bn # whether to use BatchNorm\n",
    "        self.p_prob = p_drop # dropout in probability p_drop\n",
    "\n",
    "        # Input: (B, 3, 64, 64)\n",
    "        self.block = nn.Sequential(\n",
    "            MyConvLayer(3, 32, use_bn, p_drop),     # (B, 32, 64, 64)\n",
    "            nn.MaxPool2d(2),                        # (B, 32, 32, 32)\n",
    "            MyConvLayer(32, 64, use_bn, p_drop),    # (B, 64, 32, 32)\n",
    "            nn.MaxPool2d(2),                        # (B, 64, 16, 16)\n",
    "            MyConvLayer(64, 128, use_bn, p_drop),   # (B, 128, 16, 16)\n",
    "            nn.MaxPool2d(2),                        # (B, 128, 8, 8)\n",
    "            MyConvLayer(128, 256, use_bn, p_drop),  # (B, 256, 8, 8)\n",
    "            nn.MaxPool2d(2),                        # (B, 256, 4, 4)\n",
    "            MyConvLayer(256, 512, use_bn, p_drop),  # (B, 512, 4, 4)\n",
    "            nn.MaxPool2d(2),                        # (B, 512, 2, 2)\n",
    "\n",
    "            nn.Flatten(),                           # (B, 256*2*2)\n",
    "            nn.Linear(512*2*2, 256),                # (B, 256)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(256, 10)                      # (B, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Notice: no passing through softmax here!\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the Training Function (**To be implemented by students**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=10):\n",
    "    \"\"\"\n",
    "    Train the model and measure performance.\n",
    "    - Record training time per epoch\n",
    "    - Report training loss and accuracy\n",
    "    - Measure training time per model architecture\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= total\n",
    "        train_acc = correct / total * 100\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"Epoch {epoch+1} / {epochs}, time: {epoch_end_time - epoch_start_time:.2f}s, Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Training Loss: {train_loss:.4f} | Training Acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10, time: 3.21s, Loss: 1.2289, Accuracy: 51.8380\n",
      "Epoch 2 / 10, time: 3.15s, Loss: 0.6791, Accuracy: 75.5556\n",
      "Epoch 3 / 10, time: 3.48s, Loss: 0.5189, Accuracy: 81.7130\n",
      "Epoch 4 / 10, time: 3.50s, Loss: 0.4194, Accuracy: 85.5185\n",
      "Epoch 5 / 10, time: 2.86s, Loss: 0.3378, Accuracy: 88.4074\n",
      "Epoch 6 / 10, time: 2.57s, Loss: 0.2628, Accuracy: 90.9352\n",
      "Epoch 7 / 10, time: 2.66s, Loss: 0.2170, Accuracy: 92.6296\n",
      "Epoch 8 / 10, time: 2.63s, Loss: 0.1904, Accuracy: 93.4074\n",
      "Epoch 9 / 10, time: 2.89s, Loss: 0.1935, Accuracy: 93.2315\n",
      "Epoch 10 / 10, time: 2.70s, Loss: 0.1302, Accuracy: 95.5093\n",
      "Training completed in 29.66 seconds.\n",
      "Training Loss: 0.1302 | Training Acc: 95.51%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model and move to device\n",
    "model = MyCNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3003 | Test Acc: 90.96%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_loss / total\n",
    "    test_acc = correct / total * 100\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "# Evaluate the trained model\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Additional Experiments\n",
    "\n",
    "Test whether adding Batch Normalization and Dropout layers improves the model's performance. Compare the results with the previous model without / without these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================\n",
      "Working with BatchNorm=False, Dropout=0.0\n",
      "Epoch 1 / 10, time: 2.57s, Loss: 1.1805, Accuracy: 54.2546\n",
      "Epoch 2 / 10, time: 2.86s, Loss: 0.6425, Accuracy: 76.6991\n",
      "Epoch 3 / 10, time: 2.84s, Loss: 0.4885, Accuracy: 82.8704\n",
      "Epoch 4 / 10, time: 2.75s, Loss: 0.3912, Accuracy: 86.3519\n",
      "Epoch 5 / 10, time: 3.22s, Loss: 0.3300, Accuracy: 88.5324\n",
      "Epoch 6 / 10, time: 3.51s, Loss: 0.2716, Accuracy: 90.3056\n",
      "Epoch 7 / 10, time: 3.46s, Loss: 0.2211, Accuracy: 92.2500\n",
      "Epoch 8 / 10, time: 3.02s, Loss: 0.1982, Accuracy: 92.9583\n",
      "Epoch 9 / 10, time: 3.20s, Loss: 0.1499, Accuracy: 94.7222\n",
      "Epoch 10 / 10, time: 2.87s, Loss: 0.1286, Accuracy: 95.4769\n",
      "Training completed in 30.29 seconds.\n",
      "Training Loss: 0.1286 | Training Acc: 95.48%\n",
      "Test Loss: 0.2956 | Test Acc: 90.91%\n",
      "=========================================================================\n",
      "Working with BatchNorm=True, Dropout=0.0\n",
      "Epoch 1 / 10, time: 2.71s, Loss: 0.8595, Accuracy: 69.1019\n",
      "Epoch 2 / 10, time: 2.64s, Loss: 0.5206, Accuracy: 81.7037\n",
      "Epoch 3 / 10, time: 2.56s, Loss: 0.3939, Accuracy: 86.4769\n",
      "Epoch 4 / 10, time: 2.58s, Loss: 0.3381, Accuracy: 88.3380\n",
      "Epoch 5 / 10, time: 3.34s, Loss: 0.2794, Accuracy: 90.1898\n",
      "Epoch 6 / 10, time: 3.80s, Loss: 0.2415, Accuracy: 91.5185\n",
      "Epoch 7 / 10, time: 2.87s, Loss: 0.2179, Accuracy: 92.4306\n",
      "Epoch 8 / 10, time: 2.63s, Loss: 0.1996, Accuracy: 93.0370\n",
      "Epoch 9 / 10, time: 2.53s, Loss: 0.1792, Accuracy: 93.6157\n",
      "Epoch 10 / 10, time: 2.94s, Loss: 0.1534, Accuracy: 94.8750\n",
      "Training completed in 28.59 seconds.\n",
      "Training Loss: 0.1534 | Training Acc: 94.88%\n",
      "Test Loss: 0.2242 | Test Acc: 92.83%\n",
      "=========================================================================\n",
      "Working with BatchNorm=False, Dropout=0.05\n",
      "Epoch 1 / 10, time: 3.60s, Loss: 1.3261, Accuracy: 48.0694\n",
      "Epoch 2 / 10, time: 2.83s, Loss: 0.8102, Accuracy: 70.6991\n",
      "Epoch 3 / 10, time: 2.52s, Loss: 0.6260, Accuracy: 78.0278\n",
      "Epoch 4 / 10, time: 2.65s, Loss: 0.5165, Accuracy: 81.6296\n",
      "Epoch 5 / 10, time: 3.08s, Loss: 0.4217, Accuracy: 85.4630\n",
      "Epoch 6 / 10, time: 3.17s, Loss: 0.3610, Accuracy: 87.6481\n",
      "Epoch 7 / 10, time: 3.87s, Loss: 0.3002, Accuracy: 89.5000\n",
      "Epoch 8 / 10, time: 2.88s, Loss: 0.2595, Accuracy: 91.3194\n",
      "Epoch 9 / 10, time: 2.80s, Loss: 0.2202, Accuracy: 92.5093\n",
      "Epoch 10 / 10, time: 2.67s, Loss: 0.1988, Accuracy: 93.1944\n",
      "Training completed in 30.08 seconds.\n",
      "Training Loss: 0.1988 | Training Acc: 93.19%\n",
      "Test Loss: 0.2471 | Test Acc: 91.69%\n",
      "=========================================================================\n",
      "Working with BatchNorm=True, Dropout=0.05\n",
      "Epoch 1 / 10, time: 2.69s, Loss: 0.9290, Accuracy: 66.4954\n",
      "Epoch 2 / 10, time: 2.72s, Loss: 0.6330, Accuracy: 77.7407\n",
      "Epoch 3 / 10, time: 2.87s, Loss: 0.5014, Accuracy: 82.4861\n",
      "Epoch 4 / 10, time: 3.09s, Loss: 0.4198, Accuracy: 85.5093\n",
      "Epoch 5 / 10, time: 3.18s, Loss: 0.3614, Accuracy: 87.4537\n",
      "Epoch 6 / 10, time: 2.76s, Loss: 0.3226, Accuracy: 88.7176\n",
      "Epoch 7 / 10, time: 3.75s, Loss: 0.2885, Accuracy: 89.7963\n",
      "Epoch 8 / 10, time: 3.84s, Loss: 0.2533, Accuracy: 91.0417\n",
      "Epoch 9 / 10, time: 3.35s, Loss: 0.2255, Accuracy: 92.1019\n",
      "Epoch 10 / 10, time: 2.82s, Loss: 0.2135, Accuracy: 92.6250\n",
      "Training completed in 31.07 seconds.\n",
      "Training Loss: 0.2135 | Training Acc: 92.62%\n",
      "Test Loss: 0.1967 | Test Acc: 93.13%\n"
     ]
    }
   ],
   "source": [
    "def pipeline(use_bn, p_drop):\n",
    "    print(\"=========================================================================\")\n",
    "    print(f'Working with BatchNorm={use_bn}, Dropout={p_drop}')\n",
    "\n",
    "    model = MyCNN(use_bn=use_bn, p_drop=p_drop).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_model(model, train_loader, criterion, optimizer, device, epochs=10)\n",
    "    evaluate_model(model, test_loader, device)\n",
    "\n",
    "\n",
    "# without batch normalization, without dropout\n",
    "pipeline(use_bn=False, p_drop=0.0)\n",
    "\n",
    "# with batch normalization, without dropout\n",
    "pipeline(use_bn=True, p_drop=0.0)\n",
    "\n",
    "# without batch normalization, with dropout\n",
    "pipeline(use_bn=False, p_drop=0.05)\n",
    "\n",
    "# with batch normalization, with dropout\n",
    "pipeline(use_bn=True, p_drop=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. The architecture of the CNN model we constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNN(\n",
      "  (block): Sequential(\n",
      "    (0): MyConvLayer(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): MyConvLayer(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): MyConvLayer(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): MyConvLayer(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): MyConvLayer(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Flatten(start_dim=1, end_dim=-1)\n",
      "    (11): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (12): ReLU()\n",
      "    (13): Dropout(p=0.0, inplace=False)\n",
      "    (14): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
