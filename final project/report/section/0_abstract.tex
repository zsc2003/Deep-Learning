\begin{abstract}

Recent progress in 3D computer vision increasingly favors generalization across scenes instead of per-scene optimization. A growing number of works leverage large-scale pre-training and Transformer-based architectures to support multiple 3D tasks through data-driven inference, reducing reliance on per-scene fitting and handcrafted geometric pipelines. This report reviews the designs of such methods, emphasizing how supervision signals, intermediate representations, and the degree of explicit 3D inductive bias shape both geometric reconstruction and novel view synthesis. We further discuss current limitations and highlight open directions toward unified 3D backbones that jointly support reconstruction and rendering.

\end{abstract}