\section{Introduction and Motivation}
Traditional 3D computer vision has long relied on explicit geometric solvers or per-scene optimization. While effective, these methods often struggle to generalize to unseen environments without retraining.

In contrast, the period of 2024-2025 has seen the rise of "3D Foundation Models." These models treat 3D tasks as data-driven inference problems, utilizing massive datasets to learn priors that apply across diverse scenarios.

My motivation for this project is to explore the current state-of-the-art in this domain. I aim to answer the following questions:
\begin{itemize}
    \item How do modern foundation models implicitly learn 3D geometry from large-scale 2D image collections?
    \item What are the architectural advantages of using Transformers for direct 3D regression compared to traditional pipelines?
    \item How do recent generative approaches achieve photorealistic view synthesis with minimal 3D inductive bias?
\end{itemize}